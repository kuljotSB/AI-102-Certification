{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b73e9e31",
   "metadata": {},
   "source": [
    "## Image Analysis with Azure Computer Vision Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a008eb",
   "metadata": {},
   "source": [
    "### Installing Dependencies and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4bfee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-cognitiveservices-vision-computervision==0.9.1 pillow==10.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2fd9d1",
   "metadata": {},
   "source": [
    "### Setting Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2883efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "cv_endpoint = os.getenv(\"CV_ENDPOINT\")\n",
    "cv_key = os.getenv(\"CV_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9228b4f",
   "metadata": {},
   "source": [
    "### Creating our Computer Vision Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8fabe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "\n",
    "credential = CognitiveServicesCredentials(cv_key) \n",
    "cv_client = ComputerVisionClient(cv_endpoint, credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf68edd4",
   "metadata": {},
   "source": [
    "### Specify the Features to be Retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec90ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [VisualFeatureTypes.description,\n",
    "            VisualFeatureTypes.tags,\n",
    "            VisualFeatureTypes.categories,\n",
    "            VisualFeatureTypes.brands,\n",
    "            VisualFeatureTypes.objects,\n",
    "            VisualFeatureTypes.adult,\n",
    "            VisualFeatureTypes.faces]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6d0130",
   "metadata": {},
   "source": [
    "### Get Image Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec23a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_to_analyze = [\n",
    "    \"https://raw.githubusercontent.com/kuljotSB/AI-102-Certification/main/AzureAIVision/Image-Analysis/Images/building.jpg\",\n",
    "    \"https://raw.githubusercontent.com/kuljotSB/AI-102-Certification/main/AzureAIVision/Image-Analysis/Images/person.jpg\",\n",
    "    \"https://raw.githubusercontent.com/kuljotSB/AI-102-Certification/main/AzureAIVision/Image-Analysis/Images/street.jpg\"\n",
    "]\n",
    "\n",
    "for image_url in images_to_analyze:\n",
    "    print(f\"\\nAnalyzing image: {image_url}\\n\")\n",
    "\n",
    "    # Analyze the image\n",
    "    analysis = cv_client.analyze_image(image_url, visual_features=features)\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"Description captions:\")\n",
    "    for caption in analysis.description.captions:\n",
    "        print(f\"'{caption.text}' (confidence: {caption.confidence:.2f})\")\n",
    "\n",
    "    print(\"\\nTags:\")\n",
    "    for tag in analysis.tags:\n",
    "        print(f\"{tag.name} (confidence: {tag.confidence:.2f})\")\n",
    "\n",
    "    print(\"\\nCategories:\")\n",
    "    for category in analysis.categories:\n",
    "        print(f\"{category.name} (confidence: {category.score:.2f})\")\n",
    "\n",
    "    print(\"\\nBrands:\")\n",
    "    for brand in analysis.brands:\n",
    "        print(f\"{brand.name} (confidence: {brand.confidence:.2f})\")\n",
    "\n",
    "    print(\"\\nObjects:\")\n",
    "    for obj in analysis.objects:\n",
    "        rect = obj.rectangle\n",
    "        print(f\"{obj.object_property} at location {rect.x},{rect.y},{rect.w},{rect.h} (confidence: {obj.confidence:.2f})\")\n",
    "    \n",
    "    print(\"\\nFaces:\")\n",
    "    if not analysis.faces:\n",
    "        print(\"No faces detected.\")\n",
    "    else:\n",
    "        for face in analysis.faces:\n",
    "            rect = face.face_rectangle\n",
    "            print(f\"Gender: {face.gender}, Age: {face.age}, Bounding box: \"\n",
    "                f\"left={rect.left}, top={rect.top}, width={rect.width}, height={rect.height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789a1827",
   "metadata": {},
   "source": [
    "### Use the OCR Capability to Read Handwritten Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb05d15",
   "metadata": {},
   "source": [
    "![handwritten.jpg](./Images/handwritten.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6332852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
    "from PIL import Image, ImageDraw\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "ocr_image_url = \"https://raw.githubusercontent.com/kuljotSB/AI-102-Certification/main/AzureAIVision/Image-Analysis/Images/handwritten.png\"\n",
    "\n",
    "# Download image for drawing\n",
    "response = requests.get(ocr_image_url)\n",
    "original_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "# Call API with URL and raw response to get the operation location from headers\n",
    "read_response = cv_client.read(ocr_image_url, raw=True)\n",
    "read_operation_location = read_response.headers[\"Operation-Location\"]\n",
    "operation_id = read_operation_location.split(\"/\")[-1]\n",
    "\n",
    "# Wait for the operation to complete\n",
    "while True:\n",
    "    read_result = cv_client.get_read_result(operation_id)\n",
    "    if read_result.status not in ['notStarted', 'running']:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "# Print the detected text, line by line\n",
    "if read_result.status == OperationStatusCodes.succeeded:\n",
    "    for text_result in read_result.analyze_result.read_results:\n",
    "        for line in text_result.lines:\n",
    "            print(line.text)\n",
    "            print(\"Bounding box:\", line.bounding_box)\n",
    "\n",
    "# Draw bounding boxes\n",
    "draw = ImageDraw.Draw(original_image)\n",
    "if read_result.status == OperationStatusCodes.succeeded:\n",
    "    for text_result in read_result.analyze_result.read_results:\n",
    "        for line in text_result.lines:\n",
    "            box = line.bounding_box  # [x1, y1, x2, y2, x3, y3, x4, y4]\n",
    "            # Draw a polygon bounding the text line\n",
    "            polygon = [(box[i], box[i + 1]) for i in range(0, len(box), 2)]\n",
    "            draw.polygon(polygon, outline=\"red\", width=3)\n",
    "\n",
    "# Save and display image with bounding boxes\n",
    "output_path = \"ocr_out.png\"\n",
    "original_image.save(output_path)\n",
    "original_image.show()\n",
    "print(f\"OCR visualization saved as {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
