{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be887df",
   "metadata": {},
   "source": [
    "### Azure AI Search Multi-Modal RAG - Simple Image Verbalisation Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88929718",
   "metadata": {},
   "source": [
    "![simple_image_verbalisation.png](./Assets/simple_image_verbalisation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e6e9d",
   "metadata": {},
   "source": [
    "### Installing Required Libraries and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe4f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai requests python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eb088a",
   "metadata": {},
   "source": [
    "### Setting Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d06c35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "key = os.getenv(\"AZURE_SEARCH_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec10766c",
   "metadata": {},
   "source": [
    "### Creating the Azure OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5444f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  \n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "\n",
    "azure_openai_client = AzureOpenAI(\n",
    "    api_key=azure_openai_key,\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    azure_endpoint=azure_openai_endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded31c48",
   "metadata": {},
   "source": [
    "### Creating the Embedding Generator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a421c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(client, text):\n",
    "    embedding_model = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")\n",
    "    \n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model = embedding_model\n",
    "    )\n",
    "    \n",
    "    embeddings=response.model_dump()\n",
    "    return embeddings['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542aaa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"can you tell me something about the sustainability initiatives at BMW?\"\n",
    "vectorised_user_query = generate_embeddings(azure_openai_client, user_query)\n",
    "print(vectorised_user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb26e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "context=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a117944",
   "metadata": {},
   "source": [
    "### Sending API Call to the Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb4becb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "url = f\"{service_endpoint}/indexes/{index_name}/docs/search?api-version=2023-11-01\"\n",
    "    \n",
    "headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": key\n",
    "    }\n",
    "    \n",
    "body =   {\n",
    "        \"count\": True,\n",
    "        \"select\": \"document_title, content_text, locationMetadata, image_document_id\",\n",
    "        \"vectorQueries\": [\n",
    "            {\n",
    "                \"vector\": vectorised_user_query,\n",
    "                \"k\": 10,\n",
    "                \"fields\": \"content_embedding\",\n",
    "                \"kind\": \"vector\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "response = requests.post(url, headers=headers, data=json.dumps(body))\n",
    "documents = response.json()['value']\n",
    "\n",
    "for doc in documents:\n",
    "    context.append(dict(\n",
    "        {\n",
    "            \"document_title\": doc['document_title'],\n",
    "            \"chunk\": doc['content_text'],\n",
    "            \"score\": doc['@search.score'],\n",
    "            \"locationMetadata\": doc['locationMetadata'] if 'locationMetadata' in doc else None,\n",
    "            \"image_document_id\": doc['image_document_id'] if 'image_document_id' in doc else None\n",
    "        }\n",
    "    ))\n",
    "    \n",
    "for doc in context:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0f7161",
   "metadata": {},
   "source": [
    "### Calling GPT Engine for Summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6357ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\"You are meant to behave as a RAG chatbot that derives its context from a database stored in Azure AI Search Solution.\n",
    "please answer strictly from the context from the database provided and if you dont have an answer please politely say so. dont include any extra \n",
    "information that is not in the context and dont include links as well.\n",
    "the context passed to you will be in the form of a pythonic list with each object in the list having the following structure:\n",
    "\n",
    "{{\n",
    "    \"document_title\": \"the title of the document\",\n",
    "    \"chunk\": \"the chunk of text from the document\",\n",
    "    \"score\": \"the score of the match based on cosine similarity\",\n",
    "    \"locationMetadata\": \"the location metadata if available, else None\",\n",
    "    \"image_document_id\": \"the image document id if available, else None\",\n",
    "}}\n",
    "\n",
    "the pythonic list contains best 10 matches to the user query based on cosine similarity of the embeddings of the user query and the review descriptions.\n",
    "please structure your answers in a very professional manner and in such a way that the user does not get to know that its RAG working under the hood\n",
    "and its as if they are talking to a human. \"\"\"\n",
    "\n",
    "user_prompt = f\"\"\" the user query is: {user_query}\n",
    "the context is : {context}\"\"\"\n",
    "\n",
    "chat_completions_response = azure_openai_client.chat.completions.create(\n",
    "    model = os.getenv(\"AZURE_OPENAI_CHAT_COMPLETIONS_MODEL\"),\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(chat_completions_response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
